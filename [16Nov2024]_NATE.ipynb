{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipG6Ve9QOw7n"
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "import os\n",
        "from pandas import read_csv\n",
        "from pandas import set_option\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNrkf1hmQ99Q"
      },
      "source": [
        "# test_data = pd.read_csv(\"cstest.csv\")\n",
        "train_data = pd.read_csv(\"cstrain.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idRsFNmHRrYg"
      },
      "source": [
        "train_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3aKm2yKSIne"
      },
      "source": [
        "class_0 = train_data.SeriousDlqin2yrs.value_counts()[0]\n",
        "class_1 = train_data.SeriousDlqin2yrs.value_counts()[1]\n",
        "print(\"Total number of class_0: {}\".format(class_0))\n",
        "print(\"Total number of class_1: {}\".format(class_1))\n",
        "print(\"Event rate: {} %\".format(class_1/(class_0+class_1) *100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAIL5BsmSRzE"
      },
      "source": [
        "train_data.loc[train_data[\"age\"] < 18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4-ZA7T3SVZT"
      },
      "source": [
        "train_data.loc[train_data[\"age\"] == 0, \"age\"] = train_data.age.median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU4hFbU9SjBr"
      },
      "source": [
        "age_working = train_data.loc[(train_data[\"age\"] >= 18) & (train_data[\"age\"] < 60)]\n",
        "age_senior = train_data.loc[(train_data[\"age\"] >= 60)]\n",
        "\n",
        "age_working_impute = age_working.MonthlyIncome.mean()\n",
        "age_senior_impute = age_senior.MonthlyIncome.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwTTjl_kSvP0"
      },
      "source": [
        "train_data[\"MonthlyIncome\"] = np.absolute(train_data[\"MonthlyIncome\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W27HOdYKSzRV"
      },
      "source": [
        "train_data[\"MonthlyIncome\"] = train_data[\"MonthlyIncome\"].fillna(99999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmcu3kvCS1xH"
      },
      "source": [
        "train_data[\"MonthlyIncome\"] = train_data[\"MonthlyIncome\"].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Z8g0IfS31O"
      },
      "source": [
        "train_data.loc[((train_data[\"age\"] >= 18) & (train_data[\"age\"] < 60)) & (train_data[\"MonthlyIncome\"] == 99999),\\\n",
        "               \"MonthlyIncome\"] = age_working_impute\n",
        "train_data.loc[(train_data[\"age\"] >= 60) & (train_data[\"MonthlyIncome\"] == 99999), \"MonthlyIncome\"] = age_senior_impute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q00YnOOoTEi-"
      },
      "source": [
        "train_data.loc[train_data[\"MonthlyIncome\"] == 99999]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMh0-B7T2OP"
      },
      "source": [
        "train_data[\"NumberOfDependents\"] = np.absolute(train_data[\"NumberOfDependents\"])\n",
        "train_data[\"NumberOfDependents\"] = train_data[\"NumberOfDependents\"].fillna(0)\n",
        "train_data[\"NumberOfDependents\"] = train_data[\"NumberOfDependents\"].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPuoDFPcUF07"
      },
      "source": [
        "train_data[\"CombinedDefaulted\"] = (train_data[\"NumberOfTimes90DaysLate\"] + train_data[\"NumberOfTime60-89DaysPastDueNotWorse\"])\\\n",
        "                                        + train_data[\"NumberOfTime30-59DaysPastDueNotWorse\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V6dnAiuUMn4"
      },
      "source": [
        "train_data.loc[(train_data[\"CombinedDefaulted\"] >= 1), \"CombinedDefaulted\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ENYf-1UPk8"
      },
      "source": [
        "train_data[\"CombinedCreditLoans\"] = train_data[\"NumberOfOpenCreditLinesAndLoans\"] + \\\n",
        "                                        train_data[\"NumberRealEstateLoansOrLines\"]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM85Z-B5UTnG"
      },
      "source": [
        "train_data.loc[(train_data[\"CombinedCreditLoans\"] <= 5), \"CombinedCreditLoans\"] = 0\n",
        "train_data.loc[(train_data[\"CombinedCreditLoans\"] > 5), \"CombinedCreditLoans\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC4msob9VGE3"
      },
      "source": [
        "train_data[\"WithDependents\"] = train_data[\"NumberOfDependents\"]\n",
        "train_data.loc[(train_data[\"WithDependents\"] >= 1), \"WithDependents\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHQQNQKmVPpa"
      },
      "source": [
        "train_data[\"MonthlyDebtPayments\"] = train_data[\"DebtRatio\"] * train_data[\"MonthlyIncome\"]\n",
        "train_data[\"MonthlyDebtPayments\"] = np.absolute(train_data[\"MonthlyDebtPayments\"])\n",
        "train_data[\"MonthlyDebtPayments\"] = train_data[\"MonthlyDebtPayments\"].astype('int64')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65vFJM-xVSlm"
      },
      "source": [
        "train_data[\"age\"] = train_data[\"age\"].astype('int64')\n",
        "train_data[\"MonthlyIncome\"] = train_data[\"MonthlyIncome\"].astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn8U9aapVZu6"
      },
      "source": [
        "train_data[\"age_map\"] = train_data[\"age\"]\n",
        "train_data.loc[(train_data[\"age\"] >= 18) & (train_data[\"age\"] < 60), \"age_map\"] = 1\n",
        "train_data.loc[(train_data[\"age\"] >= 60), \"age_map\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJaqkDziVdpp"
      },
      "source": [
        "train_data[\"age_map\"] = train_data[\"age_map\"].replace(0, \"working\")\n",
        "train_data[\"age_map\"] = train_data[\"age_map\"].replace(1, \"senior\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAxxrr56VjiL"
      },
      "source": [
        "train_data = pd.concat([train_data, pd.get_dummies(train_data.age_map,prefix='is')], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SdYBujpVvwF"
      },
      "source": [
        "train_data.drop([\"Unnamed: 0\",\"NumberOfOpenCreditLinesAndLoans\",\n",
        "                 \"NumberOfTimes90DaysLate\",\"NumberRealEstateLoansOrLines\",\"NumberOfTime60-89DaysPastDueNotWorse\",\n",
        "                 \"WithDependents\",\"age_map\",\"is_senior\",\"is_working\", \"MonthlyDebtPayments\"], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuuD76v2Vxje"
      },
      "source": [
        "train_data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL9DDpzXVzXN"
      },
      "source": [
        "corr = train_data.corr()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, annot=True, fmt=\".2g\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ_59gaYV3Jl"
      },
      "source": [
        "# ### Test dataset 정리 ###\n",
        "\n",
        "# def cleaned_dataset(dataset):\n",
        "#     dataset.loc[dataset[\"age\"] <= 18, \"age\"] = dataset.age.median()\n",
        "\n",
        "#     age_working = dataset.loc[(dataset[\"age\"] >= 18) & (dataset[\"age\"] < 60)]\n",
        "#     age_senior = dataset.loc[(dataset[\"age\"] >= 60)]\n",
        "\n",
        "#     age_working_impute = age_working.MonthlyIncome.mean()\n",
        "#     age_senior_impute = age_senior.MonthlyIncome.mean()\n",
        "\n",
        "#     dataset[\"MonthlyIncome\"] = np.absolute(dataset[\"MonthlyIncome\"])\n",
        "#     dataset[\"MonthlyIncome\"] = dataset[\"MonthlyIncome\"].fillna(99999)\n",
        "#     dataset[\"MonthlyIncome\"] = dataset[\"MonthlyIncome\"].astype('int64')\n",
        "\n",
        "#     dataset.loc[((dataset[\"age\"] >= 18) & (dataset[\"age\"] < 60)) & (dataset[\"MonthlyIncome\"] == 99999),\\\n",
        "#                    \"MonthlyIncome\"] = age_working_impute\n",
        "#     dataset.loc[(train_data[\"age\"] >= 60) & (dataset[\"MonthlyIncome\"] == 99999), \"MonthlyIncome\"] = age_senior_impute\n",
        "#     dataset[\"NumberOfDependents\"] = np.absolute(dataset[\"NumberOfDependents\"])\n",
        "#     dataset[\"NumberOfDependents\"] = dataset[\"NumberOfDependents\"].fillna(0)\n",
        "#     dataset[\"NumberOfDependents\"] = dataset[\"NumberOfDependents\"].astype('int64')\n",
        "\n",
        "#     dataset[\"CombinedDefaulted\"] = (dataset[\"NumberOfTimes90DaysLate\"] + dataset[\"NumberOfTime60-89DaysPastDueNotWorse\"])\\\n",
        "#                                             + dataset[\"NumberOfTime30-59DaysPastDueNotWorse\"]\n",
        "\n",
        "#     dataset.loc[(dataset[\"CombinedDefaulted\"] >= 1), \"CombinedDefaulted\"] = 1\n",
        "\n",
        "#     dataset[\"CombinedCreditLoans\"] = dataset[\"NumberOfOpenCreditLinesAndLoans\"] + \\\n",
        "#                                             dataset[\"NumberRealEstateLoansOrLines\"]\n",
        "#     dataset.loc[(dataset[\"CombinedCreditLoans\"] <= 5), \"CombinedCreditLoans\"] = 0\n",
        "#     dataset.loc[(dataset[\"CombinedCreditLoans\"] > 5), \"CombinedCreditLoans\"] = 1\n",
        "\n",
        "#     dataset.drop([\"Unnamed: 0\",\"NumberOfOpenCreditLinesAndLoans\",\\\n",
        "#                   \"NumberOfTimes90DaysLate\",\"NumberRealEstateLoansOrLines\",\"NumberOfTime60-89DaysPastDueNotWorse\"], axis=1, inplace=True)\n",
        "\n",
        "# cleaned_dataset(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrudTcwWWCps"
      },
      "source": [
        "train_data.shape\n",
        "#test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzIAAfQTWE3A"
      },
      "source": [
        "train_data.SeriousDlqin2yrs.value_counts()\n",
        "# imbalanced in target value\n",
        "# need to be changed in certain imbalanced ratio - undersampling or oversampling or ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvCP8UkwWIdI"
      },
      "source": [
        "#Create independent and depedent features - Separate features into input and output\n",
        "columns = train_data.columns.tolist()\n",
        "#Filter the columns to remove data we do not want\n",
        "columns = [c for c in columns if c not in [\"SeriousDlqin2yrs\"]]\n",
        "#Store the variable we are predicting\n",
        "target = \"SeriousDlqin2yrs\"\n",
        "#Define a random state\n",
        "state = np.random.RandomState(42)\n",
        "X = train_data[columns]\n",
        "Y = train_data[target]\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PExVn5lWLkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a384ef48-cbab-4af6-a282-6d81449f2f55"
      },
      "source": [
        "good = train_data[train_data['SeriousDlqin2yrs']==0]\n",
        "bad = train_data[train_data['SeriousDlqin2yrs']==1]\n",
        "print(good.shape, bad.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(139974, 9) (10026, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### NATE - Table 5\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# 모델 라이브러리 import\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 데이터를 준비 (X: input features, Y: target feature)\n",
        "# 예시: X, Y 는 실제 데이터로 대체되어야 함\n",
        "# X = pd.DataFrame(...) # 8개 input feature로 구성된 데이터프레임\n",
        "# Y = pd.Series(...)    # target feature\n",
        "\n",
        "# 사용할 분류 모델들\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# 5-fold 교차 검증 설정\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 평가 기준 정의\n",
        "scoring = {\n",
        "           'accuracy': 'accuracy',\n",
        "           'AUC': make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr'),\n",
        "           'MCC': make_scorer(matthews_corrcoef),\n",
        "           'F1':'f1'\n",
        "           }\n",
        "\n",
        "# 결과를 저장할 리스트\n",
        "results = []\n",
        "\n",
        "# 각 모델에 대해 교차 검증 수행\n",
        "for name, model in models.items():\n",
        "    cv_results = cross_validate(model, X, Y, cv=kf, scoring=scoring)\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy Mean': np.mean(cv_results['test_accuracy']),\n",
        "        'Accuracy Std': np.std(cv_results['test_accuracy']),\n",
        "        'AUC Mean': np.mean(cv_results['test_AUC']),\n",
        "        'AUC Std': np.std(cv_results['test_AUC']),\n",
        "        'MCC Mean': np.mean(cv_results['test_MCC']),\n",
        "        'MCC Std': np.std(cv_results['test_MCC']),\n",
        "        'F1 Mean': np.mean(cv_results['test_F1']),\n",
        "        'F1 Std': np.std(cv_results['test_F1'])\n",
        "    })\n",
        "\n",
        "# 결과를 데이터프레임으로 변환하여 출력\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "ifAQJyfwishG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NATE - Table 6 - SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score, f1_score\n",
        "# from sklearn.utils.fixes import loguniform\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # 경고 무시\n",
        "\n",
        "# 데이터 준비\n",
        "# 실제 데이터로 대체 필요\n",
        "# 예시로 랜덤 데이터 생성 (8개의 피처, 불균형 타겟)\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# X, Y = make_classification(n_samples=1000, n_features=8,\n",
        "#                            n_informative=5, n_redundant=2,\n",
        "#                            n_clusters_per_class=2, weights=[0.93, 0.07],\n",
        "#                            flip_y=0, random_state=42)\n",
        "\n",
        "# SMOTE를 적용할 불균형 비율 설정\n",
        "desired_ratios = [0.15, 0.32, 0.50, 1.00]  # 15%, 32%, 50%, 100%\n",
        "\n",
        "# 모델별 하이퍼파라미터 그리드 정의\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 1000],\n",
        "        'max_features': ['auto', 'sqrt'],\n",
        "        'max_depth': [1, 20],\n",
        "        'min_samples_split': [2, 5, 10],  # min_samples_split cannot be 1\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 1000],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_depth': [1, 20],\n",
        "        'min_samples_split': [2, 5, 10],  # min_samples_split cannot be 1\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'subsample': [0.8, 1.0]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 1000],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_depth': [1, 20],\n",
        "        'min_child_weight': [2, 5, 10],  # 대체 min_samples_split\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]  # 대체 min_samples_leaf\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
        "        # 'C': loguniform(1e-5, 100),\n",
        "        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
        "    }\n",
        "}\n",
        "\n",
        "# 평가 지표 정의\n",
        "scoring = {\n",
        "    'AUC': 'roc_auc',\n",
        "    'MCC': make_scorer(matthews_corrcoef),\n",
        "    'F1': 'f1'\n",
        "}\n",
        "\n",
        "# 결과를 저장할 리스트 초기화\n",
        "results = []\n",
        "\n",
        "# K-Fold 교차 검증 설정\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 각 불균형 비율에 대해 SMOTE 적용 및 모델 학습\n",
        "for ratio in desired_ratios:\n",
        "    print(f\"\\n=== SMOTE 적용: Minority 비율 {int(ratio*100)}% ===\")\n",
        "\n",
        "    # SMOTE 적용\n",
        "    smote = SMOTE(sampling_strategy=ratio, random_state=42)\n",
        "    X_res, Y_res = smote.fit_resample(X, Y)\n",
        "\n",
        "    print(f\"Resampled dataset shape: {np.bincount(Y_res)}\")\n",
        "\n",
        "    for model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Logistic Regression']:\n",
        "        print(f\"\\n--- 모델: {model_name} ---\")\n",
        "\n",
        "        # 모델 초기화\n",
        "        if model_name == 'Random Forest':\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "        elif model_name == 'Gradient Boosting':\n",
        "            model = GradientBoostingClassifier(random_state=42)\n",
        "        elif model_name == 'XGBoost':\n",
        "            model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "        elif model_name == 'Logistic Regression':\n",
        "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "        # 하이퍼파라미터 그리드 설정\n",
        "        param_grid = param_grids[model_name]\n",
        "\n",
        "        # GridSearchCV 설정\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grid,\n",
        "            scoring=scoring,\n",
        "            refit='AUC',  # AUC를 기준으로 최적 모델 선택\n",
        "            cv=kf,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Grid Search 수행\n",
        "        grid_search.fit(X_res, Y_res)\n",
        "\n",
        "        # 최적 모델의 교차 검증 결과 추출\n",
        "        cv_results = grid_search.cv_results_\n",
        "        best_index = grid_search.best_index_\n",
        "\n",
        "        auc_scores = cv_results['mean_test_AUC']\n",
        "        mcc_scores = cv_results['mean_test_MCC']\n",
        "        f1_scores = cv_results['mean_test_F1']\n",
        "\n",
        "        best_auc = cv_results['mean_test_AUC'][best_index]\n",
        "        best_mcc = cv_results['mean_test_MCC'][best_index]\n",
        "        best_f1 = cv_results['mean_test_F1'][best_index]\n",
        "\n",
        "        # 결과 저장\n",
        "        results.append({\n",
        "            'SMOTE Ratio (%)': int(ratio * 100),\n",
        "            'Model': model_name,\n",
        "            'Best AUC': best_auc,\n",
        "            'Best MCC': best_mcc,\n",
        "            'Best F1 Score': best_f1,\n",
        "            'Best Parameters': grid_search.best_params_\n",
        "        })\n",
        "\n",
        "        print(f\"Best AUC: {best_auc:.4f}, Best MCC: {best_mcc:.4f}, Best F1: {best_f1:.4f}\")\n",
        "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 결과를 DataFrame으로 변환\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n=== 최종 결과 ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "Sgbl43wDs9la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NATE - Table 6 - NearMiss\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score, f1_score\n",
        "# from sklearn.utils.fixes import loguniform\n",
        "\n",
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # 경고 무시\n",
        "\n",
        "# 데이터 준비\n",
        "# 실제 데이터로 대체 필요\n",
        "# 예시로 랜덤 데이터 생성 (8개의 피처, 불균형 타겟)\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# X, Y = make_classification(n_samples=1000, n_features=8,\n",
        "#                            n_informative=5, n_redundant=2,\n",
        "#                            n_clusters_per_class=2, weights=[0.93, 0.07],\n",
        "#                            flip_y=0, random_state=42)\n",
        "\n",
        "# NearMiss를 적용할 불균형 비율 설정\n",
        "desired_ratios = [0.15, 0.32, 0.50, 1.00]  # 15%, 32%, 50%\n",
        "\n",
        "# 모델별 하이퍼파라미터 그리드 정의\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 1000],\n",
        "        'max_features': ['auto', 'sqrt'],\n",
        "        'max_depth': [1, 20],\n",
        "        'min_samples_split': [2, 5, 10],  # min_samples_split cannot be 1\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 1000],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_depth': [1, 20],\n",
        "        'min_samples_split': [2, 5, 10],  # min_samples_split cannot be 1\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'subsample': [0.8, 1.0]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 1000],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_depth': [1, 20],\n",
        "        'min_child_weight': [2, 5, 10],  # 대체 min_samples_split\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]  # 대체 min_samples_leaf\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
        "        # 'C': loguniform(1e-5, 100),\n",
        "        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
        "    }\n",
        "}\n",
        "\n",
        "# 평가 지표 정의\n",
        "scoring = {\n",
        "    'AUC': 'roc_auc',\n",
        "    'MCC': make_scorer(matthews_corrcoef),\n",
        "    'F1': 'f1'\n",
        "}\n",
        "\n",
        "# 결과를 저장할 리스트 초기화\n",
        "results = []\n",
        "\n",
        "# K-Fold 교차 검증 설정\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 각 불균형 비율에 대해 NearMiss 적용 및 모델 학습\n",
        "for ratio in desired_ratios:\n",
        "    print(f\"\\n=== NearMiss 적용: Minority 비율 {int(ratio*100)}% ===\")\n",
        "\n",
        "    # NearMiss 적용\n",
        "    nearmiss = NearMiss(sampling_strategy=ratio)\n",
        "    X_res, Y_res = nearmiss.fit_resample(X, Y)\n",
        "\n",
        "    print(f\"Resampled dataset shape: {np.bincount(Y_res)}\")\n",
        "\n",
        "    for model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'Logistic Regression']:\n",
        "        print(f\"\\n--- 모델: {model_name} ---\")\n",
        "\n",
        "        # 모델 초기화\n",
        "        if model_name == 'Random Forest':\n",
        "            model = RandomForestClassifier(random_state=42)\n",
        "        elif model_name == 'Gradient Boosting':\n",
        "            model = GradientBoostingClassifier(random_state=42)\n",
        "        elif model_name == 'XGBoost':\n",
        "            model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "        elif model_name == 'Logistic Regression':\n",
        "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "        # 하이퍼파라미터 그리드 설정\n",
        "        param_grid = param_grids[model_name]\n",
        "\n",
        "        # GridSearchCV 설정\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grid,\n",
        "            scoring=scoring,\n",
        "            refit='AUC',  # AUC를 기준으로 최적 모델 선택\n",
        "            cv=kf,\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Grid Search 수행\n",
        "        grid_search.fit(X_res, Y_res)\n",
        "\n",
        "        # 최적 모델의 교차 검증 결과 추출\n",
        "        cv_results = grid_search.cv_results_\n",
        "        best_index = grid_search.best_index_\n",
        "\n",
        "        auc_scores = cv_results['mean_test_AUC']\n",
        "        mcc_scores = cv_results['mean_test_MCC']\n",
        "        f1_scores = cv_results['mean_test_F1']\n",
        "\n",
        "        best_auc = cv_results['mean_test_AUC'][best_index]\n",
        "        best_mcc = cv_results['mean_test_MCC'][best_index]\n",
        "        best_f1 = cv_results['mean_test_F1'][best_index]\n",
        "\n",
        "        # 결과 저장\n",
        "        results.append({\n",
        "            'NearMiss Ratio (%)': int(ratio * 100),\n",
        "            'Model': model_name,\n",
        "            'Best AUC': best_auc,\n",
        "            'Best MCC': best_mcc,\n",
        "            'Best F1 Score': best_f1,\n",
        "            'Best Parameters': grid_search.best_params_\n",
        "        })\n",
        "\n",
        "        print(f\"Best AUC: {best_auc:.4f}, Best MCC: {best_mcc:.4f}, Best F1: {best_f1:.4f}\")\n",
        "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 결과를 DataFrame으로 변환\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n=== 최종 결과 ===\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "mnb0er_ois14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BEST model - GB using SMOTE - Table 8\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# # Data preparation\n",
        "# X, Y = make_classification(n_samples=1000, n_features=8,\n",
        "#                            n_informative=5, n_redundant=2,\n",
        "#                            n_clusters_per_class=2, weights=[0.93, 0.07],\n",
        "#                            flip_y=0, random_state=42)\n",
        "\n",
        "# Apply SMOTE with desired ratio\n",
        "smote = SMOTE(sampling_strategy=1.00, random_state=42)\n",
        "X_res, Y_res = smote.fit_resample(X, Y)\n",
        "\n",
        "# Define Gradient Boosting model with best hyperparameters\n",
        "best_params = {\n",
        "    'n_estimators': 1000,\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 20,\n",
        "    'min_samples_leaf': 4,\n",
        "    'min_samples_split': 10,\n",
        "    'subsample': 0.8,\n",
        "    'random_state': 42\n",
        "}\n",
        "gb_best_model = GradientBoostingClassifier(**best_params)\n",
        "\n",
        "# K-Fold cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Scoring functions\n",
        "scoring = {\n",
        "    'AUC': make_scorer(roc_auc_score, needs_proba=True),\n",
        "    'MCC': make_scorer(matthews_corrcoef),\n",
        "    'F1': make_scorer(f1_score)\n",
        "}\n",
        "\n",
        "# Dictionary to store results and computation time for each metric\n",
        "results = {}\n",
        "for metric_name, scorer in scoring.items():\n",
        "    start_time = time.time()\n",
        "    scores = cross_val_score(gb_best_model, X_res, Y_res, scoring=scorer, cv=kf, n_jobs=-1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    results[metric_name] = {\n",
        "        'Score': scores.mean(),\n",
        "        'Time (s)': elapsed_time\n",
        "    }\n",
        "\n",
        "# Output the results\n",
        "print(\"\\n=== Best Model Evaluation Results ===\")\n",
        "for metric, result in results.items():\n",
        "    print(f\"{metric}: {result['Score']:.4f} (Time taken: {result['Time (s)']:.2f} seconds)\")\n"
      ],
      "metadata": {
        "id": "O2mewnpCis7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### BEST model - XGB using NearMiss - Table 9\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score, f1_score\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# # Data preparation (generate synthetic dataset)\n",
        "# X, Y = make_classification(n_samples=1000, n_features=8,\n",
        "#                            n_informative=5, n_redundant=2,\n",
        "#                            n_clusters_per_class=2, weights=[0.93, 0.07],\n",
        "#                            flip_y=0, random_state=42)\n",
        "\n",
        "# Apply NearMiss with desired ratio 1.00 (full undersampling)\n",
        "nearmiss = NearMiss(sampling_strategy=1.00)\n",
        "X_res, Y_res = nearmiss.fit_resample(X, Y)\n",
        "\n",
        "# Define XGBoost model with specified best hyperparameters\n",
        "best_params = {\n",
        "    'n_estimators': 1000,\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 20,\n",
        "    'min_child_weight': 10,\n",
        "    'subsample': 1.0,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'use_label_encoder': False,\n",
        "    'eval_metric': 'logloss'\n",
        "}\n",
        "xgb_best_model = XGBClassifier(**best_params)\n",
        "\n",
        "# K-Fold cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Scoring functions\n",
        "scoring_functions = {\n",
        "    'AUC': make_scorer(roc_auc_score, needs_proba=True),\n",
        "    'MCC': make_scorer(matthews_corrcoef),\n",
        "    'F1': make_scorer(f1_score)\n",
        "}\n",
        "\n",
        "# Dictionary to store results and computation time for each metric\n",
        "results = {}\n",
        "for metric_name, scorer in scoring_functions.items():\n",
        "    start_time = time.time()\n",
        "    scores = cross_val_score(xgb_best_model, X_res, Y_res, scoring=scorer, cv=kf, n_jobs=-1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    results[metric_name] = {\n",
        "        'Score': scores.mean(),\n",
        "        'Time (s)': elapsed_time\n",
        "    }\n",
        "\n",
        "# Output the results\n",
        "print(\"\\n=== Best Model Evaluation Results ===\")\n",
        "for metric, result in results.items():\n",
        "    print(f\"{metric}: {result['Score']:.4f} (Time taken: {result['Time (s)']:.2f} seconds)\")\n"
      ],
      "metadata": {
        "id": "TO6K-UZ7is9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest with 50 % bad in total\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "forest = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators=1000,\n",
        "    max_depth=20,\n",
        "    max_features=\"auto\",\n",
        "    min_samples_leaf=1,\n",
        "    min_samples_split=2,\n",
        "    class_weight=\"balanced\",\n",
        "    bootstrap=False\n",
        ")\n",
        "\n",
        "smote = SMOTE(sampling_strategy=1.00, random_state=42)\n",
        "X_res, Y_res = smote.fit_resample(X, Y)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_res,y_res,random_state=42)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, Y, test_size=0.30, random_state=42)\n",
        "\n",
        "y_scores_proba = forest.predict_proba(X_train)\n",
        "y_scores = y_scores_proba[:,1]\n"
      ],
      "metadata": {
        "id": "coCpOO2hPV73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "ylTww4CmPWCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_to_show = 5\n",
        "data_for_prediction1 = X_test2.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
        "data_for_prediction_array_notdefaulted = data_for_prediction1.values.reshape(1, -1)\n",
        "\n",
        "\n",
        "forest.predict_proba(data_for_prediction_array_notdefaulted)"
      ],
      "metadata": {
        "id": "poq5Bi3kPWI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_to_show = 22\n",
        "data_for_prediction2 = X_test2.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
        "data_for_prediction_array_defaulted = data_for_prediction2.values.reshape(1, -1)\n",
        "\n",
        "forest.predict_proba(data_for_prediction_array_defaulted)"
      ],
      "metadata": {
        "id": "c3ConSKmSgBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap  # package used to calculate Shap values\n",
        "\n",
        "# Create object that can calculate shap values\n",
        "explainer = shap.TreeExplainer(forest)\n",
        "\n",
        "# Calculate Shap values\n",
        "shap_values = explainer.shap_values(data_for_prediction1)"
      ],
      "metadata": {
        "id": "edFUaxhOPWNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shap force plot - not defaulted\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction1)"
      ],
      "metadata": {
        "id": "BjhaeyBxSW6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shap force plot - defaulted\n",
        "shap.initjs()\n",
        "shap_values2 = explainer.shap_values(data_for_prediction2)\n",
        "shap.force_plot(explainer.expected_value[1], shap_values2[1], data_for_prediction2)"
      ],
      "metadata": {
        "id": "asVOLzGpSW9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shap decision plot -  Not defaluted\n",
        "shap.initjs()\n",
        "shap.decision_plot(explainer.expected_value[1], shap_values[1], data_for_prediction1, feature_display_range=slice(None, -16, -1))"
      ],
      "metadata": {
        "id": "XiZroKVRSXAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shap decision plot -  defaluted\n",
        "shap.initjs()\n",
        "shap.decision_plot(explainer.expected_value[1], shap_values2[1], data_for_prediction2, feature_display_range=slice(None, -16, -1))"
      ],
      "metadata": {
        "id": "tppQGkpzSo9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global explanation\n",
        "\n",
        "X_importance = X_test2\n",
        "\n",
        "# Explain model predictions using shap library:\n",
        "explainer = shap.TreeExplainer()\n",
        "shap_values = explainer.shap_values(X_importance)"
      ],
      "metadata": {
        "id": "hpIWcfIySpAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. feature importance plot - Global interpretability\n",
        "import shap\n",
        "\n",
        "shap_values = shap.TreeExplainer(forest).shap_values(X_test2)\n",
        "shap.summary_plot(shap_values, X_test2, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "qHj2TTWHSpGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shap summary_plot\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_importance)"
      ],
      "metadata": {
        "id": "4gNqyXSGSpJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}